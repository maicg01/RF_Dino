{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class DiscreteDeepQ(object):\n",
    "    def __init__(self, observation_size,\n",
    "                       num_actions,\n",
    "                       observation_to_actions,\n",
    "                       optimizer,\n",
    "                       session,\n",
    "                       random_action_probability=0.05,\n",
    "                       exploration_period=1000,\n",
    "                       store_every_nth=5,\n",
    "                       train_every_nth=5,\n",
    "                       minibatch_size=32,\n",
    "                       discount_rate=0.95,\n",
    "                       max_experience=30000,\n",
    "                       target_network_update_rate=0.01,\n",
    "                       summary_writer=None):\n",
    "        \"\"\"Initialized the Deepq object.\n",
    "\n",
    "        Based on:\n",
    "            https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        observation_size : int\n",
    "            length of the vector passed as observation\n",
    "        num_actions : int\n",
    "            number of actions that the model can execute\n",
    "        observation_to_actions: dali model\n",
    "            model that implements activate function\n",
    "            that can take in observation vector or a batch\n",
    "            and returns scores (of unbounded values) for each\n",
    "            action for each observation.\n",
    "            input shape:  [batch_size, observation_size]\n",
    "            output shape: [batch_size, num_actions]\n",
    "        optimizer: tf.solver.*\n",
    "            optimizer for prediction error\n",
    "        session: tf.Session\n",
    "            session on which to execute the computation\n",
    "        random_action_probability: float (0 to 1)\n",
    "        exploration_period: int\n",
    "            probability of choosing a random\n",
    "            action (epsilon form paper) annealed linearly\n",
    "            from 1 to random_action_probability over\n",
    "            exploration_period\n",
    "        store_every_nth: int\n",
    "            to further decorrelate samples do not all\n",
    "            transitions, but rather every nth transition.\n",
    "            For example if store_every_nth is 5, then\n",
    "            only 20% of all the transitions is stored.\n",
    "        train_every_nth: int\n",
    "            normally training_step is invoked every\n",
    "            time action is executed. Depending on the\n",
    "            setup that might be too often. When this\n",
    "            variable is set set to n, then only every\n",
    "            n-th time training_step is called will\n",
    "            the training procedure actually be executed.\n",
    "        minibatch_size: int\n",
    "            number of state,action,reward,newstate\n",
    "            tuples considered during experience reply\n",
    "        dicount_rate: float (0 to 1)\n",
    "            how much we care about future rewards.\n",
    "        max_experience: int\n",
    "            maximum size of the reply buffer\n",
    "        target_network_update_rate: float\n",
    "            how much to update target network after each\n",
    "            iteration. Let's call target_network_update_rate\n",
    "            alpha, target network T, and network N. Every\n",
    "            time N gets updated we execute:\n",
    "                T = (1-alpha)*T + alpha*N\n",
    "        summary_writer: tf.train.SummaryWriter\n",
    "            writer to log metrics\n",
    "        \"\"\"\n",
    "        # memorize arguments\n",
    "        self.observation_size          = observation_size\n",
    "        self.num_actions               = num_actions\n",
    "\n",
    "        self.q_network                 = observation_to_actions\n",
    "        self.optimizer                 = optimizer\n",
    "        self.s                         = session\n",
    "\n",
    "        self.random_action_probability = random_action_probability\n",
    "        self.exploration_period        = exploration_period\n",
    "        self.store_every_nth           = store_every_nth\n",
    "        self.train_every_nth           = train_every_nth\n",
    "        self.minibatch_size            = minibatch_size\n",
    "        self.discount_rate             = tf.constant(discount_rate)\n",
    "        self.max_experience            = max_experience\n",
    "        self.target_network_update_rate = \\\n",
    "                tf.constant(target_network_update_rate)\n",
    "\n",
    "        # deepq state\n",
    "        self.actions_executed_so_far = 0\n",
    "        self.experience = deque()\n",
    "\n",
    "        self.iteration = 0\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        self.number_of_times_store_called = 0\n",
    "        self.number_of_times_train_called = 0\n",
    "\n",
    "        self.create_variables()\n",
    "\n",
    "    def linear_annealing(self, n, total, p_initial, p_final):\n",
    "        \"\"\"Linear annealing between p_initial and p_final\n",
    "        over total steps - computes value at step n\"\"\"\n",
    "        if n >= total:\n",
    "            return p_final\n",
    "        else:\n",
    "            return p_initial - (n * (p_initial - p_final)) / (total)\n",
    "\n",
    "    def create_variables(self):\n",
    "        self.target_q_network    = self.q_network.copy(scope=\"target_network\")\n",
    "\n",
    "        # FOR REGULAR ACTION SCORE COMPUTATION\n",
    "        with tf.name_scope(\"taking_action\"):\n",
    "            self.observation        = tf.placeholder(tf.float32, (None, self.observation_size), name=\"observation\")\n",
    "            self.action_scores      = tf.identity(self.q_network(self.observation), name=\"action_scores\")\n",
    "            tf.histogram_summary(\"action_scores\", self.action_scores)\n",
    "            self.predicted_actions  = tf.argmax(self.action_scores, dimension=1, name=\"predicted_actions\")\n",
    "\n",
    "        with tf.name_scope(\"estimating_future_rewards\"):\n",
    "            # FOR PREDICTING TARGET FUTURE REWARDS\n",
    "            self.next_observation          = tf.placeholder(tf.float32, (None, self.observation_size), name=\"next_observation\")\n",
    "            self.next_observation_mask     = tf.placeholder(tf.float32, (None,), name=\"next_observation_mask\")\n",
    "            self.next_action_scores        = tf.stop_gradient(self.target_q_network(self.next_observation))\n",
    "            tf.histogram_summary(\"target_action_scores\", self.next_action_scores)\n",
    "            self.rewards                   = tf.placeholder(tf.float32, (None,), name=\"rewards\")\n",
    "            target_values                  = tf.reduce_max(self.next_action_scores, reduction_indices=[1,]) * self.next_observation_mask\n",
    "            self.future_rewards            = self.rewards + self.discount_rate * target_values\n",
    "\n",
    "        with tf.name_scope(\"q_value_precition\"):\n",
    "            # FOR PREDICTION ERROR\n",
    "            self.action_mask                = tf.placeholder(tf.float32, (None, self.num_actions), name=\"action_mask\")\n",
    "            self.masked_action_scores       = tf.reduce_sum(self.action_scores * self.action_mask, reduction_indices=[1,])\n",
    "            temp_diff                       = self.masked_action_scores - self.future_rewards\n",
    "            self.prediction_error           = tf.reduce_mean(tf.square(temp_diff))\n",
    "            gradients                       = self.optimizer.compute_gradients(self.prediction_error)\n",
    "            for i, (grad, var) in enumerate(gradients):\n",
    "                if grad is not None:\n",
    "                    gradients[i] = (tf.clip_by_norm(grad, 5), var)\n",
    "            # Add histograms for gradients.\n",
    "            for grad, var in gradients:\n",
    "                tf.histogram_summary(var.name, var)\n",
    "                if grad is not None:\n",
    "                    tf.histogram_summary(var.name + '/gradients', grad)\n",
    "            self.train_op                   = self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "        # UPDATE TARGET NETWORK\n",
    "        with tf.name_scope(\"target_network_update\"):\n",
    "            self.target_network_update = []\n",
    "            for v_source, v_target in zip(self.q_network.variables(), self.target_q_network.variables()):\n",
    "                # this is equivalent to target = (1-alpha) * target + alpha * source\n",
    "                update_op = v_target.assign_sub(self.target_network_update_rate * (v_target - v_source))\n",
    "                self.target_network_update.append(update_op)\n",
    "            self.target_network_update = tf.group(*self.target_network_update)\n",
    "\n",
    "        # summaries\n",
    "        tf.scalar_summary(\"prediction_error\", self.prediction_error)\n",
    "\n",
    "        self.summarize = tf.merge_all_summaries()\n",
    "        self.no_op1    = tf.no_op()\n",
    "\n",
    "    def action(self, observation):\n",
    "        \"\"\"Given observation returns the action that should be chosen using\n",
    "        DeepQ learning strategy. Does not backprop.\"\"\"\n",
    "        assert len(observation.shape) == 1, \\\n",
    "                \"Action is performed based on single observation.\"\n",
    "\n",
    "        self.actions_executed_so_far += 1\n",
    "        exploration_p = self.linear_annealing(self.actions_executed_so_far,\n",
    "                                              self.exploration_period,\n",
    "                                              1.0,\n",
    "                                              self.random_action_probability)\n",
    "\n",
    "        if random.random() < exploration_p:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            return self.s.run(self.predicted_actions, {self.observation: observation[np.newaxis,:]})[0]\n",
    "\n",
    "    def store(self, observation, action, reward, newobservation):\n",
    "        \"\"\"Store experience, where starting with observation and\n",
    "        execution action, we arrived at the newobservation and got thetarget_network_update\n",
    "        reward reward\n",
    "\n",
    "        If newstate is None, the state/action pair is assumed to be terminal\n",
    "        \"\"\"\n",
    "        if self.number_of_times_store_called % self.store_every_nth == 0:\n",
    "            self.experience.append((observation, action, reward, newobservation))\n",
    "            if len(self.experience) > self.max_experience:\n",
    "                self.experience.popleft()\n",
    "        self.number_of_times_store_called += 1\n",
    "\n",
    "    def training_step(self):\n",
    "        \"\"\"Pick a self.minibatch_size exeperiences from reply buffer\n",
    "        and backpropage the value function.\n",
    "        \"\"\"\n",
    "        if self.number_of_times_train_called % self.train_every_nth == 0:\n",
    "            if len(self.experience) <  self.minibatch_size:\n",
    "                return\n",
    "\n",
    "            # sample experience.\n",
    "            samples   = random.sample(range(len(self.experience)), self.minibatch_size)\n",
    "            samples   = [self.experience[i] for i in samples]\n",
    "\n",
    "            # bach states\n",
    "            states         = np.empty((len(samples), self.observation_size))\n",
    "            newstates      = np.empty((len(samples), self.observation_size))\n",
    "            action_mask    = np.zeros((len(samples), self.num_actions))\n",
    "\n",
    "            newstates_mask = np.empty((len(samples),))\n",
    "            rewards        = np.empty((len(samples),))\n",
    "\n",
    "            for i, (state, action, reward, newstate) in enumerate(samples):\n",
    "                states[i] = state\n",
    "                action_mask[i] = 0\n",
    "                action_mask[i][action] = 1\n",
    "                rewards[i] = reward\n",
    "                if newstate is not None:\n",
    "                    newstates[i] = newstate\n",
    "                    newstates_mask[i] = 1\n",
    "                else:\n",
    "                    newstates[i] = 0\n",
    "                    newstates_mask[i] = 0\n",
    "\n",
    "\n",
    "            calculate_summaries = self.iteration % 100 == 0 and \\\n",
    "                    self.summary_writer is not None\n",
    "\n",
    "            cost, _, summary_str = self.s.run([\n",
    "                self.prediction_error,\n",
    "                self.train_op,\n",
    "                self.summarize if calculate_summaries else self.no_op1,\n",
    "            ], {\n",
    "                self.observation:            states,\n",
    "                self.next_observation:       newstates,\n",
    "                self.next_observation_mask:  newstates_mask,\n",
    "                self.action_mask:            action_mask,\n",
    "                self.rewards:                rewards,\n",
    "            })\n",
    "\n",
    "            self.s.run(self.target_network_update)\n",
    "\n",
    "            if calculate_summaries:\n",
    "                self.summary_writer.add_summary(summary_str, self.iteration)\n",
    "\n",
    "            self.iteration += 1\n",
    "\n",
    "        self.number_of_times_train_called += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
