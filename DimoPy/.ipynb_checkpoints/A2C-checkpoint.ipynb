{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chrome://dino/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autopy import *\n",
    "import gc\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from matplotlib import cm\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32gui, win32com.client\n",
    "import re\n",
    "\n",
    "class WindowMgr:\n",
    "    \"\"\"Encapsulates some calls to the winapi for window management\"\"\"\n",
    "    def __init__ (self):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        self._handle = None\n",
    "\n",
    "    def find_window(self, class_name, window_name = None):\n",
    "        \"\"\"find a window by its class_name\"\"\"\n",
    "        self._handle = win32gui.FindWindow(class_name, window_name)\n",
    "\n",
    "    def _window_enum_callback(self, hwnd, wildcard):\n",
    "        '''Pass to win32gui.EnumWindows() to check all the opened windows'''\n",
    "        if re.match(wildcard, str(win32gui.GetWindowText(hwnd))) != None:\n",
    "            self._handle = hwnd\n",
    "\n",
    "    def find_window_wildcard(self, wildcard):\n",
    "        self._handle = None\n",
    "        win32gui.EnumWindows(self._window_enum_callback, wildcard)\n",
    "        \n",
    "   \n",
    "    def find_window_position_size(self):\n",
    "        rect = win32gui.GetWindowRect(self._handle)\n",
    "        x = rect[0]\n",
    "        y = rect[1]\n",
    "        w = rect[2] - x\n",
    "        h = rect[3] - y\n",
    "        return (x , y , w , h)\n",
    "    \n",
    "    def set_foreground(self):\n",
    "        \"\"\"put the window in the foreground\"\"\"\n",
    "        shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "        shell.SendKeys('%')\n",
    "        win32gui.SetForegroundWindow(self._handle)\n",
    "        \n",
    "    def get_screen_image(self):\n",
    "        x , y , w , h = self.find_window_position_size()\n",
    "        bmp = bitmap.capture_screen()\n",
    "        bmp.save('screen.png')\n",
    "        img = Image.open('screen.png')\n",
    "        img = img.crop((x + 10 , y + 150  , x+w -10 , y+ h - 200 ))\n",
    "        img = img.resize((256,256)).convert('L')\n",
    "        img = ImageOps.invert(img)\n",
    "        shape = [(0, 55), (w, 50)]\n",
    "        img = np.asarray(img)\n",
    "        img = img / 255.0\n",
    "        img = img.reshape((256,256))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(image):\n",
    "    avg = np.average(image[52:55 , 0:255])\n",
    "    if avg < 0.05:\n",
    "        return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_press(code):\n",
    "        key.toggle(code , True)\n",
    "        key.toggle(code , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_tap(code):\n",
    "    key.tap(code , delay = 0.00000001)\n",
    "#     key.toggle(code , True)\n",
    "#     time.sleep(0.000001)\n",
    "#     key.toggle(code , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym:\n",
    "    def __init__(self):\n",
    "        self.window = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.window = WindowMgr()\n",
    "        self.window.find_window_wildcard(\".*chrome://dino/.*\")\n",
    "        self.window.set_foreground()\n",
    "        key_press(key.Code.F5)\n",
    "        time.sleep(0.08)\n",
    "        key_press(key.Code.SPACE)\n",
    "        img = np.zeros((256,256,3))\n",
    "        return img\n",
    "    \n",
    "    def step(self , action):\n",
    "        \n",
    "        \n",
    "        if action == 2 :\n",
    "            key_press(key.Code.UP_ARROW)\n",
    "        if action == 0 :\n",
    "            key_tap(key.Code.UP_ARROW)\n",
    "        if action == 3 :\n",
    "            key_press(key.Code.DOWN_ARROW)\n",
    "        if action == 1 :\n",
    "            pass\n",
    "#         delay = 0 if 0.1 - delay <=0 else 0.1 - delay\n",
    "#         time.sleep(delay)\n",
    "        img1 = self.window.get_screen_image()\n",
    "        time.sleep(1/50)\n",
    "        img2 = self.window.get_screen_image()\n",
    "        time.sleep(1/50)\n",
    "        img3 = self.window.get_screen_image()\n",
    "        \n",
    "        img = np.stack([img1 , img2 , img3] , axis=-1)\n",
    "\n",
    "        done = is_done(img3)\n",
    "\n",
    "\n",
    "        if done : \n",
    "            reward =  +1\n",
    "        else:\n",
    "            reward = -1\n",
    "  \n",
    "        return img , done , reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gym()\n",
    "# env.reset()\n",
    "# done = True\n",
    "# while done :\n",
    "#     observation,done,reward = env.step(np.random.randint(0,3))\n",
    "# #     im = Image.fromarray(np.uint8(cm.gist_earth(observation)*255))\n",
    "# #     display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iDQNAgent:\n",
    "    def __init__(self, state_shape , action_no, memory_size = 1500):\n",
    "        self.action_no = action_no\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        self.learning_rate = 1e-4\n",
    "        self.value_c = 0.5\n",
    "        self.entropy_c = 1e-4\n",
    "        self.clip_ratio = 0.1\n",
    "        self.std_adv = True\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        \n",
    "        self.model = self.build_model(state_shape , action_no)\n",
    " \n",
    "        \n",
    "        ####################################################\n",
    "    def build_model(self ,input_shape , n_outputs):\n",
    "        inputs = layers.Input(shape = input_shape , name='state')\n",
    "        x = layers.Conv2D(64, (4,4) , strides=4 ,padding='same' , kernel_initializer='he_normal')(inputs)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (4,4) , strides=2 ,padding='same' , kernel_initializer='he_normal')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (3,3) , strides=2 ,padding='same' , kernel_initializer='he_normal')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "#         x = layers.MaxPooling2D()(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (3,3) , strides=2 ,padding='same' , kernel_initializer='he_normal')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "#         x = layers.MaxPooling2D()(x)\n",
    "        \n",
    "        x = layers.Conv2D(64, (3,3) , strides=2 ,padding='same' , kernel_initializer='he_normal')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "#         x = layers.MaxPooling2D()(x)\n",
    "   \n",
    "        x = layers.Flatten()(x) \n",
    "\n",
    "        x = layers.Dense(512 , activation='relu')(x)\n",
    "        x = layers.Dense(256 , activation='relu')(x)\n",
    "        \n",
    "        value = layers.Dense(1 , activation='linear' , name = 'value')(x)\n",
    "        logits = layers.Dense(n_outputs , activation='linear' , name = 'policy_logits')(x)\n",
    "        \n",
    "        model = Model(inputs , [value,logits])\n",
    "#         model.summary()\n",
    "        return model\n",
    "    \n",
    "        #####################################################\n",
    "    def act(self , state):\n",
    "        value, logits = self.model.predict(state)\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "        return logits, action, np.squeeze(value, axis=-1)\n",
    "        #####################################################\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value, standardize_adv):\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            if dones[t] == True: \n",
    "                returns[t] = rewards[t] + self.gamma * returns[t + 1] \n",
    "            else:\n",
    "                returns[t] = rewards[t]\n",
    "                \n",
    "        returns = returns[:-1]\n",
    "\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "        \n",
    "        if standardize_adv:\n",
    "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-10)\n",
    "\n",
    "        return returns, advantages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 1 # 1000000\n",
    "time_steps =    100\n",
    "batch_size = 16 #64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = iDQNAgent((256 , 256 , 3) ,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = np.empty((batch_size), dtype=np.int32)\n",
    "rewards, dones, values = np.empty((3, batch_size))\n",
    "states = np.empty((batch_size,) + agent.state_shape)\n",
    "old_logits = np.empty((batch_size, agent.action_no), dtype=np.float32)\n",
    "\n",
    "ep_rewards = [0.0]\n",
    "next_obs = env.reset()\n",
    "\n",
    "for episode in range(episode_count):\n",
    "\n",
    "    for step in range(batch_size) :\n",
    "        states[step] = next_state.copy()\n",
    "        old_logits[step], actions[step], values[step] = agent.act(next_obs[None, :])\n",
    "        next_obs, rewards[step], dones[step] = env.step(actions[step])\n",
    "        ep_rewards[-1] += rewards[step]\n",
    "        if dones[step] == False:\n",
    "            ep_rewards.append(0.0)\n",
    "            next_obs = env.reset()\n",
    "            \n",
    "    _, _, next_value = agent.act(next_obs[None, :])\n",
    "\n",
    "    returns, advs = agent._returns_advantages(rewards, dones, values, next_value, agent.std_adv)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        v, logits = agent.model.predict(states, training=True)\n",
    "\n",
    "        logit_loss = self._logits_loss_a2c(actions, advs, logits)\n",
    "\n",
    "        value_loss = self._value_loss(returns, v)\n",
    "        loss = logit_loss + value_loss\n",
    "    \n",
    "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = True\n",
    "while done :\n",
    "    q_values = agent.q_model.predict(np.expand_dims(state , axis=0))\n",
    "    action = np.argmax(q_values[0])\n",
    "    next_state , done ,reward  = env.step(action ,0)\n",
    "    state = next_state\n",
    "#     im = Image.fromarray(np.uint8(cm.gist_earth(observation)*255))\n",
    "#     display(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
